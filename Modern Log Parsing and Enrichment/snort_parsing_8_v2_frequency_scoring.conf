# Author: Justin Henderson
#         SANS Instructor and author of SANS SEC555: SIEM and Tactical Analytics
# Updated by: Doug Burks
# Last Update: 8/17/2017

input {
    stdin {}
}

filter {
    # This is used to capture the current time before filter section processing takes place
    ruby {
        code => "event.set('task_start',Time.now.to_f)"
    }
    # This is the initial parsing of the log
    grok {
        match => ["message", "^\[%{INT:gid:int}:%{INT:sid:int}:%{INT:rev}\]\s%{DATA:alert}\[Classification:\s+%{DATA:classification}\]\s+\[Priority:\s+%{INT:priority:int}\]:\s+<%{DATA:interface}>\s+{%{DATA:protocol}}\s+%{IPV4:source_ip}:%{INT:source_port:int}\s+->\s+%{IPV4:destination_ip}:%{INT:destination_port:int}$",
        "message", "^\[%{INT:gid:int}:%{INT:sid:int}:%{INT:rev}\]\s%{DATA:alert}\[Classification:\s+%{DATA:classification}\]\s+\[Priority:\s+%{INT:priority:int}\]:\s+<%{DATA:interface}>\s+{%{DATA:protocol}}\s%{IPV4:source_ip}\s+->\s+%{IPV4:destination_ip}$", 
        "message", "^\[%{INT:gid:int}:%{INT:sid:int}:%{INT:rev}\]\s%{DATA:alert}\[Classification:\s+%{DATA:classification}\]\s+\[Priority:\s+%{INT:priority:int}\]:\s+{%{DATA:protocol}}\s+%{IPV4:source_ip}:%{INT:source_port:int}\s+->\s+%{IPV4:destination_ip}:%{INT:destination_port:int}$", 
        "message", "^\[%{INT:gid:int}:%{INT:sid:int}:%{INT:rev}\]\s%{DATA:alert}\[Classification:\s+%{DATA:classification}\]\s+\[Priority:\s+%{INT:priority:int}\]:\s+{%{DATA:protocol}}\s%{IPV4:source_ip}\s+->\s+%{IPV4:destination_ip}$"]
    }

    # If the alert is a Snort GPL alert break it apart for easier reading and categorization
    if [alert] =~ "GPL " {
        # This will parse out the category type from the alert
        grok {
            match => { "alert" => "GPL\s+%{DATA:category}\s" }
        }
        # This will store the category
        mutate {
            add_field => { "rule_type" => "Snort GPL" }
            lowercase => [ "category"]
        }
    }
    # If the alert is an Emerging Threat alert break it apart for easier reading and categorization
    if [alert] =~ "ET " {
        # This will parse out the category type from the alert
        grok {
            match => { "alert" => "ET\s+%{DATA:category}\s" }
        }
        # This will store the category
        mutate {
            add_field => { "rule_type" => "Emerging Threats" }
            lowercase => [ "category"]
        }
    }
    
    # This will translate the priority field into a severity field of either High, Medium, or Low
    if [priority] == 1 {
        mutate {
            add_field => { "severity" => "High" }
        }
    }
    if [priority] == 2 {
        mutate {
            add_field => { "severity" => "Medium" }
        }
    }
    if [priority] == 3 {
        mutate {
            add_field => { "severity" => "Low" }
        }
    }
    # This will perform a standard geoip lookup on the source and destination IP addresses
    geoip {
        source => "destination_ip"
        target => "destination_geo"
        tag_on_failure => []
    }
    geoip {
        source => "source_ip"
        target => "source_geo"
        tag_on_failure => []
    }
    # This will perform a geoip ASN lookup on the source and destination IP addresses
    geoip {
        source => "destination_ip"
        target => "destination_geo"
        default_database_type => "ASN"
        tag_on_failure => []
    }
    geoip {
        source => "source_ip"
        target => "source_geo"
        default_database_type => "ASN"
        tag_on_failure => []
    }
    # This will look into previous DNS logs to find if any DNS name query resolved to
    # the destination IP from the snort alert.
    memoize {
        key => "%{destination_ip}"
        fields => [ "highest_registered_domain", "query" ]
        filter_name => "elasticsearch"
        filter_options => {
            query => "type:bro_dns AND answers:%{destination_ip}"
            index => "logstash-*"
            fields => {
                "highest_registered_domain" => "destination_highest_registered_domain"
                "query" => "destination_fqdn"
            }
        }
    }
    memoize {
        key => "%{source_ip}"
        fields => [ "highest_registered_domain", "query" ]
        filter_name => "elasticsearch"
        filter_options => {
            query => "type:bro_dns AND answers:%{source_ip}"
            index => "logstash-*"
            fields => {
                "highest_registered_domain" => "source_highest_registered_domain"
                "query" => "source_fqdn"
            }
        }
    }
    # This will look into previous HTTP logs to find if any key file types have been downloaded
    memoize {
        key => "%{destination_ip}"
        fields => [ "resp_mime_types", "useragent" ]
        filter_name => "elasticsearch"
        filter_options => {
            query => "type:bro_http AND destination_ip:%{destination_ip} AND (uri:*.exe OR uri:*.pdf OR uri:*.jar OR *.doc OR *.xls OR *.zip)"
            index => "logstash-*"
            fields => {
                "resp_mime_types" => "resp_mime_types"
                "useragent" => "useragent"
                "uri" => "uri"
            }
        }
    }
    memoize {
        key => "%{source_ip}"
        fields => [ "resp_mime_types", "useragent" ]
        filter_name => "elasticsearch"
        filter_options => {
            query => "type:bro_http AND destination_ip:%{source_ip} AND (uri:*.exe OR uri:*.pdf OR uri:*.jar OR *.doc OR *.xls OR *.zip)"
            index => "logstash-*"
            fields => {
                "resp_mime_types" => "resp_mime_types"
                "useragent" => "useragent"
                "uri" => "uri"
            }
        }
    }
    if [source_highest_registered_domain] {
        memoize {
            key => "%{source_highest_registered_domain}"
            fields => [ "rank" ]
            filter_name => "elasticsearch"
            filter_options => {
                query => "highest_registered_domain:%{source_highest_registered_domain}"
                index => "alexa-top1m"
                fields => {
                    "rank" => "rank"
                }
            }
        }
    }
    if [destination_highest_registered_domain] {
        memoize {
            key => "%{destination_highest_registered_domain}"
            fields => [ "rank" ]
            filter_name => "elasticsearch"
            filter_options => {
                query => "highest_registered_domain:%{destination_highest_registered_domain}"
                index => "alexa-top1m"
                fields => {
                    "rank" => "rank"
                }
            }
        }
    }
    if [rank] {
        mutate  {
            add_tag => "top-1m"
        }
    }
    # This section will calculate the natural language frequency score of a domain
    # assuming the domain is not a top-1m domain and a domain exists
    if "top-1m" not in [tags] and [source_highest_registered_domain] {
        memoize {
            key => "%{source_highest_registered_domain}"
            fields => [ "frequency_score" ]
            filter_name => "rest"
            filter_options => {
                request => {
                    url => "http://localhost:10005/measure/%{source_highest_registered_domain}"
                }
                sprintf => true
                json => false
                target => "source_frequency_score"
            }
        }
    }
    if "top-1m" not in [tags] and [destination_highest_registered_domain] {
        memoize {
            key => "%{destination_highest_registered_domain}"
            fields => [ "frequency_score" ]
            filter_name => "rest"
            filter_options => {
                request => {
                    url => "http://localhost:10005/measure/%{destination_highest_registered_domain}"
                }
                sprintf => true
                json => false
                target => "destination_frequency_score"
            }
        }
    }
    # This will capture the finish time of the filter processing section and then use it with the
    # start time to calculate how long the log took to process.
    ruby {
        code => "event.set('task_end',Time.now.to_f)"
    }
    ruby {
        code => "event.set('logstash_time',(event.get('task_end') - event.get('task_start')).round(8))"
    }
    mutate {
        remove_field => [ 'task_start', 'task_end' ]
    }
}

output {
    stdout { codec => rubydebug }
    file {
        path => "/tmp/snort_parsing_8_frequency_scoring.log"
        codec => "json"
    }
}